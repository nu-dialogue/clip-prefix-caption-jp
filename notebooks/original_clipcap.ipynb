{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 準備"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 ライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リポジトリ\n",
    "!git clone https://github.com/nu-dialogue/clip-prefix-caption-jp.git\n",
    "%cd clip-prefix-caption-jp\n",
    "\n",
    "\n",
    "# 必要ライブラリインストール\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install git+https://github.com/rinnakk/japanese-clip.git\n",
    "!pip install scikit-image torch transformers sentencepiece"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import argparse\n",
    "from IPython.display import display\n",
    "from google.colab import files\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from preprocess import prepare_data\n",
    "from inference import Predictor\n",
    "from model import build_models_from_pretrained\n",
    "from train import set_default_args_to_parser, train\n",
    "\n",
    "# 後で使う関数も作っておく\n",
    "def upload_file():\n",
    "  uploaded = files.upload()\n",
    "  if not uploaded:\n",
    "    image_fpath = ''\n",
    "  elif len(uploaded) == 1:\n",
    "    image_fpath = list(uploaded.keys())[0]\n",
    "  else:\n",
    "    raise RuntimeError(\"1度に1枚まで\")\n",
    "  return image_fpath"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 自分のデータセットを用意"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 データの設置"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data/ディレクトリに以下のデータを用意\n",
    "```\n",
    "data/\n",
    "  └original/ # データセット名\n",
    "    ├images/ # 画像データを含んだフォルダ\n",
    "    │  ├001.jpeg # 画像ファイル名は何でもよい（連番である必要はない）\n",
    "    │  ├002.jpeg\n",
    "    │  └...\n",
    "    │\n",
    "    └captions.csv # 画像ファイル名とそのキャプション文のペアリスト\n",
    "```\n",
    "\n",
    "captions.csvの中身\n",
    "```csv\n",
    "001.jpeg,スケボーに興じる一人の男性がいます。\n",
    "002.jpeg,ゲレンデでスキーをしている人がいます。\n",
    "...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 データの前処理\n",
    "1.1で用意したデータセットについて，以下の処理を施す\n",
    "- CLIPモデルを使用して各画像を埋め込み表現に変換しておく\n",
    "- 全事例をtraining/validation/test に分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data(clip_model_name=\"en_clip_b32\",\n",
    "             captions_fpath=\"data/original/captions.csv\",\n",
    "             image_dpath=\"data/original/images\",\n",
    "             test_ratio=0.1,\n",
    "             valid_ratio=0.1,\n",
    "             train_ratio=0.8,\n",
    "             shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 自分が用意したデータでfine-tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 COCOで事前学習済みのモデルをダウンロード\n",
    "COCOデータセットで事前に学習された重みでモデルを初期化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_repo_id = \"nu-dialogue/sfcoco2022-clipcap\"\n",
    "_ = hf_hub_download(repo_id=pretrained_repo_id, filename=\"coco-gpt_medium-en_clip_b32-transformer-finetune-ep10-bs48-lr2e-05/args.json\")\n",
    "coco_checkpoint_fpath = hf_hub_download(repo_id=pretrained_repo_id, filename=\"coco-gpt_medium-en_clip_b32-transformer-finetune-ep10-bs48-lr2e-05/004.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 学習\n",
    "学習パラメータのtips\n",
    "- (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習の引数を設定\n",
    "parser = argparse.ArgumentParser()\n",
    "set_default_args_to_parser(parser)\n",
    "args = parser.parse_args(args=[\n",
    "    '--train_name_prefix', \"coco_based\",\n",
    "    '--dataset_name', 'original',\n",
    "    '--rinna_gpt_name', 'gpt_medium',\n",
    "    '--clip_model_name', 'en_clip_b32',\n",
    "    '--pretrained_path', coco_checkpoint_fpath,\n",
    "    '--epochs', '20',\n",
    "    '--per_gpu_train_batch_size', '8',\n",
    "    '--save_every', '2',\n",
    "    '--n_gpu', '1'\n",
    "    # その他パラメータはset_default_args_to_parser()を参照せよ\n",
    "])\n",
    "\n",
    "# 学習実行\n",
    "# ベストモデルのパスがbest_pt_fpathに格納される\n",
    "_, best_pt_fpath = train(args=args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Fine-tunedモデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_model, cap_tokenizer, clip_model, clip_preprocess = build_models_from_pretrained(best_pt_fpath)\n",
    "original_predictor = Predictor(cap_model=cap_model, cap_tokenizer=cap_tokenizer,\n",
    "                               clip_model=clip_model, clip_preprocess=clip_preprocess)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 テスト画像の選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### 好きな画像をアップロードする場合\n",
    "#@markdown ローカルにある画像を使いたい場合は，このセルを実行してアップロードしてください．\n",
    "#@markdown アップロードした画像はカレントディレクトリ直下に吐き出されます．\n",
    "\n",
    "image_fpath = upload_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### テスト画像リストから選ぶ場合\n",
    "#@markdown テスト画像リストの画像を使用する場合は，このセルを実行して1枚選択してください．\n",
    "\n",
    "# テスト画像ファイルリスト読込\n",
    "TEST_IMAGE_FNAME_LIST = json.load(open(\"data/original/processed-en_clip_b32/test_list.json\"))\n",
    "\n",
    "# 1枚選択\n",
    "image_fname = TEST_IMAGE_FNAME_LIST[0]\n",
    "# image_fname = random.choice(TEST_IMAGE_FNAME_LIST)\n",
    "\n",
    "image_fpath = os.path.join(\"data/original/images\", image_fname)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 キャプション生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# キャプション生成\n",
    "pil_image, captions = original_predictor.caption(image_fpath=image_fpath, beam_size=5)\n",
    "display(pil_image) # 画像を表示\n",
    "print(*captions, sep=\"\\n\") # キャプションを表示"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9 (default, Jul 12 2021, 16:21:20) \n[GCC 8.3.1 20191121 (Red Hat 8.3.1-5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "200c693d8882c277af210aff1ba4dc30fecef434f6eeccb3ce09a43e1b416df9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
